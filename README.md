# ProtFrag - Protein Fragment Prediction from pLM Embeddings

<p align="center">
  <img src="logo.png" width="180" alt="logo" />
</p>

## Overview

This project implements a multi-task deep learning model to predict protein fragments from ProtT5 embeddings.

The model performs two related tasks:

1. **Binary Classification**: Predicts if a sequence is Complete vs. Fragment
2. **Multilabel Classification**: Predicts the type of fragment (N-terminal, C-terminal, Internal gaps)

This repository provides a complete pipeline â€” from raw UniProt data parsing and redundancy reduction to embedding generation, model training, and evaluation.

---

## ğŸš€ Repository Structure
```
.
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml              # Hyperparameters for data, model, training
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ embeddings/               # Stores [entry].pt embedding files
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ clustered/            # Output of MMseqs2
â”‚   â”‚   â”œâ”€â”€ metadata_raw.csv      # Output of step 1 (parsing)
â”‚   â”‚   â””â”€â”€ metadata.csv          # Output of step 3 (splits)
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ fragments.fasta       # Raw UniProt downloads
â”‚       â”œâ”€â”€ complete.fasta
â”‚       â””â”€â”€ fragment_annotations.tsv
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_parse_uniprot_data.py          # Parses FASTA/TSV â†’ metadata_raw.csv
â”‚   â”œâ”€â”€ 02_run_mmseqs.sh                  # Clusters sequences for redundancy
â”‚   â”œâ”€â”€ 03_create_train_val_test_splits.py # Creates final metadata.csv
â”‚   â””â”€â”€ 04_precompute_embeddings.py       # Generates embeddings
â”‚
â”œâ”€â”€ src/                          # All Python source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data.py                   # PyTorch Dataset and DataModule
â”‚   â”œâ”€â”€ metrics.py                # Custom MCC and Multilabel metrics
â”‚   â”œâ”€â”€ model.py                  # The FragmentDetector LightningModule
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ fragment_parser.py    # Core logic for parsing NON_TER/NON_CONS
â”‚
â”œâ”€â”€ checkpoints/                  # Saved model .ckpt files
â”œâ”€â”€ lightning_logs/               # TensorBoard logs
â”œâ”€â”€ results/                      # Evaluation outputs (plots, predictions.csv)
â”‚
â”œâ”€â”€ train.py                      # Main training script
â”œâ”€â”€ evaluate.py                   # Main evaluation script
â”œâ”€â”€ requirements.txt              # Project dependencies
â”œâ”€â”€ QUICKSTART.md                 # Step-by-step tutorial
â””â”€â”€ README.md                     # This file
```

---

## ğŸ—ï¸ Model Architecture

The model is a multi-task classifier with a shared backbone:
```
Input: ProtT5 Embedding (1024-dim)
    â†“
Shared Encoder:
    Linear(1024 â†’ 512) + BatchNorm + ReLU + Dropout
    Linear(512 â†’ 256) + BatchNorm + ReLU + Dropout
    â†“                     â†“
Binary Head           Multilabel Head
(1 neuron)            (3 neurons)
    â†“                     â†“
Complete/Fragment    [N-term, C-term, Internal]
```

### Loss Function

The total loss is a weighted sum of the two task losses. Class weights are used to handle data imbalance:

$$L_{total} = w_b \cdot L_{BCE}(binary) + w_m \cdot L_{BCE}(multilabel)$$

---

## ğŸ’¡ Key Design Decisions

- **Multi-task Learning**: A shared encoder learns common fragment features, while separate heads specialize
- **Redundancy Reduction**: `scripts/02_run_mmseqs.sh` is used to cluster sequences and ensure the test set is not "contaminated" with sequences highly similar to the training set
- **Correct C-Terminal Parsing**: `src/utils/fragment_parser.py` correctly uses sequence length to differentiate N-terminal, C-terminal, and internal NON_TER annotations
- **Multilabel (Not Multiclass)**: The fragment type head is multilabel (sigmoid on 3 neurons), as fragments can have multiple incompleteness types simultaneously
- **Stratified Splitting**: `scripts/03_...` creates reproducible splits from the non-redundant set, stratified by both fragment status and sequence length bins
- **Robust Evaluation**: The primary metric is Matthews Correlation Coefficient (MCC), suitable for imbalanced datasets
- **Config-Driven**: All hyperparameters, paths, and training settings are controlled via `configs/default.yaml` for easy experimentation

---

## âš¡ Usage

For a complete step-by-step guide, see **QUICKSTART.md**.

### General Workflow
```bash
# 1. Download Data
# (Run the wget commands in the quickstart to populate data/raw/)

# 2. Parse Data
python scripts/01_parse_uniprot_data.py

# 3. Reduce Redundancy
bash scripts/02_run_mmseqs.sh

# 4. Create Splits
# (This script automatically finds the output from step 3)
python scripts/03_create_train_val_test_splits.py

# 5. Generate Embeddings (requires GPU)
# (This script reads the final metadata.csv from step 4)
python scripts/04_precompute_embeddings.py

# 6. Train Model
python train.py --config configs/default.yaml

# 7. Evaluate Model
python evaluate.py --checkpoint [path_to_checkpoint.ckpt]
```

---

## ğŸ©º Troubleshooting

### ğŸ§  OutOfMemoryError (OOM)

- Reduce `data.batch_size` in `configs/default.yaml`
- Set `training.precision: 16` for mixed-precision

### ğŸ“‚ Embeddings Not Found

- Ensure `data/embeddings/` contains a `.pt` file for every entry in `data/processed/metadata.csv`
- Re-run `scripts/04_precompute_embeddings.py` if the data changed

### ğŸ“‰ Poor Convergence (Low val/binary_mcc)

- Try decreasing `model.learning_rate` (e.g., to 0.0001)
- Increase `model.dropout` if overfitting occurs (train loss << val loss)