{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precompute Per-Residue Embeddings using ProtT5\n",
    "\n",
    "This notebook generates protein embeddings using ProtT5 optimized for **Google Colab T4 GPU**.\n",
    "\n",
    "**Before starting:**\n",
    "1. **Enable GPU**: Runtime → Change runtime type → T4 GPU\n",
    "2. Run cells in order from top to bottom\n",
    "\n",
    "**Memory optimization for T4 (15GB VRAM):**\n",
    "- Uses half precision (fp16) to reduce memory usage\n",
    "- Processes sequences efficiently\n",
    "- Aggressive memory cleanup between sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch pandas numpy tqdm sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google.colab import files\n",
    "\n",
    "# Configuration optimized for T4 GPU\n",
    "OUTPUT_DIR = \"embeddings\"\n",
    "MODEL_NAME = \"Rostlab/prot_t5_xl_half_uniref50-enc\"  # Half precision model for T4\n",
    "MAX_LENGTH = 1000  # Reduced for T4 memory constraints\n",
    "USE_FP16 = True  # Use half precision to save memory\n",
    "\n",
    "print(\"✓ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda:0')\n",
    "    print(f\"✓ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"✓ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"⚠ No GPU detected! This will be very slow. Please enable GPU in Runtime settings.\")\n",
    "    USE_FP16 = False\n",
    "\n",
    "print(f\"\\nUsing device: {DEVICE}\")\n",
    "print(f\"Half precision (FP16): {USE_FP16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_spaced(seq: str) -> str:\n",
    "    \"\"\"Convert sequence to space-separated letters for ProtT5.\"\"\"\n",
    "    return \" \".join(list(seq.strip()))\n",
    "\n",
    "print(\"✓ Helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Your CSV File\n",
    "\n",
    "Your CSV must have columns: `id` and `sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please upload your CSV file (must have 'id' and 'sequence' columns):\")\n",
    "uploaded = files.upload()\n",
    "INPUT_CSV = list(uploaded.keys())[0]\n",
    "print(f\"\\n✓ Uploaded file: {INPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "print(f\"✓ Loaded {len(df)} sequences\")\n",
    "print(f\"✓ Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "display(df.head(3))\n",
    "\n",
    "# Validate columns\n",
    "if 'id' not in df.columns or 'sequence' not in df.columns:\n",
    "    raise ValueError(\"CSV must contain 'id' and 'sequence' columns!\")\n",
    "\n",
    "# Check sequence lengths\n",
    "seq_lengths = df['sequence'].str.len()\n",
    "print(f\"\\nSequence length statistics:\")\n",
    "print(f\"  Min: {seq_lengths.min()}\")\n",
    "print(f\"  Max: {seq_lengths.max()}\")\n",
    "print(f\"  Mean: {seq_lengths.mean():.1f}\")\n",
    "print(f\"  Median: {seq_lengths.median():.1f}\")\n",
    "n_long = (seq_lengths > MAX_LENGTH).sum()\n",
    "if n_long > 0:\n",
    "    print(f\"\\n⚠ Warning: {n_long} sequences will be truncated to {MAX_LENGTH} residues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load ProtT5 Model\n",
    "\n",
    "This may take 2-3 minutes on first run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take 2-3 minutes on first run...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "    print(\"✓ Tokenizer loaded\")\n",
    "    \n",
    "    # Load model with memory optimization\n",
    "    model = T5EncoderModel.from_pretrained(MODEL_NAME)\n",
    "    print(\"✓ Model downloaded\")\n",
    "    \n",
    "    # Move to GPU and set to eval mode\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(\"✓ Model moved to device\")\n",
    "    \n",
    "    # Use half precision if on GPU\n",
    "    if USE_FP16:\n",
    "        model = model.half()\n",
    "        print(\"✓ Using half precision (FP16)\")\n",
    "    \n",
    "    print(f\"\\n✓ Model loaded successfully!\")\n",
    "    print(f\"✓ Model dtype: {next(model.parameters()).dtype}\")\n",
    "    \n",
    "    # Check GPU memory after loading\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        print(f\"✓ GPU memory allocated: {allocated:.2f} GB\")\n",
    "        print(f\"✓ GPU memory reserved: {reserved:.2f} GB\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error loading model: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure GPU is enabled (Runtime → Change runtime type → T4 GPU)\")\n",
    "    print(\"2. Try restarting the runtime (Runtime → Restart runtime)\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Embeddings\n",
    "\n",
    "This is the main processing step. It may take several minutes depending on the number of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processing {len(df)} sequences...\\n\")\n",
    "\n",
    "failed_sequences = []\n",
    "\n",
    "for idx, row in enumerate(tqdm(df.itertuples(index=False), total=len(df), desc=\"Processing\")):\n",
    "    try:\n",
    "        seq = str(getattr(row, \"sequence\")).strip()\n",
    "        sid = str(getattr(row, \"id\"))\n",
    "        \n",
    "        # Skip empty sequences\n",
    "        if not seq or len(seq) == 0:\n",
    "            print(f\"\\n⚠ Skipping empty sequence: {sid}\")\n",
    "            failed_sequences.append((sid, \"empty sequence\"))\n",
    "            continue\n",
    "        \n",
    "        # Truncate if sequence is too long\n",
    "        if len(seq) > MAX_LENGTH:\n",
    "            seq = seq[:MAX_LENGTH]\n",
    "        \n",
    "        # Prepare input\n",
    "        spaced_seq = seq_to_spaced(seq)\n",
    "        ids = tokenizer.encode_plus(spaced_seq, add_special_tokens=True, padding=\"longest\")\n",
    "        input_ids = torch.tensor(ids['input_ids']).unsqueeze(0)\n",
    "        attention_mask = torch.tensor(ids['attention_mask']).unsqueeze(0)\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_mask = attention_mask.to(DEVICE)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            emb = outputs.last_hidden_state.squeeze(0).cpu()\n",
    "            \n",
    "            # Convert back to float32 for saving if using fp16\n",
    "            if USE_FP16:\n",
    "                emb = emb.float()\n",
    "            \n",
    "            emb = emb.numpy()\n",
    "        \n",
    "        # Save embeddings\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"{sid}.npy\")\n",
    "        np.save(output_path, emb)\n",
    "        \n",
    "        # Aggressive memory cleanup\n",
    "        del input_ids, attention_mask, outputs, emb\n",
    "        \n",
    "        # Clear cache every 10 sequences\n",
    "        if (idx + 1) % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error processing sequence {sid}: {e}\")\n",
    "        failed_sequences.append((sid, str(e)))\n",
    "        continue\n",
    "\n",
    "# Final cleanup\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n✓ Embeddings saved to {OUTPUT_DIR}/\")\n",
    "\n",
    "# Report failures\n",
    "if failed_sequences:\n",
    "    print(f\"\\n⚠ Failed to process {len(failed_sequences)} sequences:\")\n",
    "    for sid, error in failed_sequences[:5]:  # Show first 5\n",
    "        print(f\"  - {sid}: {error}\")\n",
    "    if len(failed_sequences) > 5:\n",
    "        print(f\"  ... and {len(failed_sequences) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.npy')]\n",
    "print(f\"✓ Generated {len(embedding_files)} embedding files\")\n",
    "print(f\"✓ Success rate: {len(embedding_files)}/{len(df)} ({len(embedding_files)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if embedding_files:\n",
    "    sample_file = embedding_files[0]\n",
    "    sample_emb = np.load(os.path.join(OUTPUT_DIR, sample_file))\n",
    "    print(f\"\\nSample embedding: {sample_file}\")\n",
    "    print(f\"  Shape: {sample_emb.shape}\")\n",
    "    print(f\"  Dtype: {sample_emb.dtype}\")\n",
    "    print(f\"  Size: {sample_emb.nbytes / 1024:.1f} KB\")\n",
    "    print(f\"  Embedding dimension: {sample_emb.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating zip file...\")\n",
    "zip_filename = \"embeddings\"\n",
    "shutil.make_archive(zip_filename, 'zip', OUTPUT_DIR)\n",
    "zip_size = os.path.getsize(f\"{zip_filename}.zip\") / (1024 * 1024)\n",
    "print(f\"✓ Zip file created: {zip_filename}.zip ({zip_size:.1f} MB)\")\n",
    "print(\"\\nDownloading...\")\n",
    "files.download(f\"{zip_filename}.zip\")\n",
    "print(\"\\n✓ DONE! All embeddings have been generated and downloaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}